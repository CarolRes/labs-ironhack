{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Bag of words (BoW)** is an important technique in text mining and [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval). BoW uses term-frequency vectors to represent the content of text documents which makes it possible to use mathematics and computer programs to analyze and compare text documents.\n",
    "\n",
    "BoW contains the following information:\n",
    "\n",
    "1. A dictionary of all the terms (words) in the text documents. The terms are normalized in terms of the letter case (e.g. `Ironhack` => `ironhack`), tense (e.g. `had` => `have`), singular form (e.g. `students` => `student`), etc.\n",
    "1. The number of occurrences of each normalized term in each document.\n",
    "\n",
    "For example, assume we have three text documents:\n",
    "\n",
    "DOC 1: **Ironhack is cool.**\n",
    "\n",
    "DOC 2: **I love Ironhack.**\n",
    "\n",
    "DOC 3: **I am a student at Ironhack.**\n",
    "\n",
    "The BoW of the above documents looks like below:\n",
    "\n",
    "| TERM | DOC 1 | DOC 2 | Doc 3 |\n",
    "|---|---|---|---|\n",
    "| a | 0 | 0 | 1 |\n",
    "| am | 0 | 0 | 1 |\n",
    "| at | 0 | 0 | 1 |\n",
    "| cool | 1 | 0 | 0 |\n",
    "| i | 0 | 1 | 1 |\n",
    "| ironhack | 1 | 1 | 1 |\n",
    "| is | 1 | 0 | 0 |\n",
    "| love | 0 | 1 | 0 |\n",
    "| student | 0 | 0 | 1 |\n",
    "\n",
    "\n",
    "The term-frequency array of each document in BoW can be considered a high-dimensional vector. Data scientists use these vectors to represent the content of the documents. For instance, DOC 1 is represented with `[0, 0, 0, 1, 0, 1, 1, 0, 0]`, DOC 2 is represented with `[0, 0, 0, 0, 1, 1, 0, 1, 0]`, and DOC 3 is represented with `[1, 1, 1, 0, 1, 1, 0, 0, 1]`. **Two documents are considered identical if their vector representations have close [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).**\n",
    "\n",
    "In real practice there are many additional techniques to improve the text mining accuracy such as using [stop words](https://en.wikipedia.org/wiki/Stop_words) (i.e. neglecting common words such as `a`, `I`, `to` that don't contribute much meaning), synonym list (e.g. consider `New York City` the same as `NYC` and `Big Apple`), and HTML tag removal if the data sources are webpages. In Module 3 you will learn how to use those advanced techniques for [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), a component of text mining.\n",
    "\n",
    "In real text mining projects data analysts use packages such as Scikit-Learn and NLTK, which you will learn in Module 3, to extract BoW from texts. In this exercise, however, we would like you to create BoW manually with Python. This is because by manually creating BoW you can better understand the concept and also practice the Python skills you have learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "We need to create a BoW from a list of documents. The documents (`doc1.txt`, `doc2.txt`, and `doc3.txt`) can be found in the `your-code` directory of this exercise. You will read the content of each document into an array of strings named `corpus`.\n",
    "\n",
    "*What is a corpus (plural: corpora)? Read the reference in the README file.*\n",
    "\n",
    "Your challenge is to use Python to generate the BoW of these documents. Your BoW should look like below:\n",
    "\n",
    "```python\n",
    "bag_of_words = ['a', 'am', 'at', 'cool', 'i', 'ironhack', 'is', 'love', 'student']\n",
    "\n",
    "term_freq = [\n",
    "    [0, 0, 0, 1, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1, 0, 1, 0],\n",
    "    [1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
    "]\n",
    "```\n",
    "\n",
    "Now let's define the `docs` array that contains the paths of `doc1.txt`, `doc2.txt`, and `doc3.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['doc1.txt', 'doc2.txt', 'doc3.txt']\n",
    "\n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ironhack is cool.', 'I love Ironhack.', 'I am a student at Ironhack.']\n"
     ]
    }
   ],
   "source": [
    "# open a file repleting the open function several times\n",
    "\n",
    "for text in docs:\n",
    "    f = (open(text))\n",
    "    corpus.append(f.read())\n",
    "    \n",
    "      \n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an empty array `corpus` that will contain the content strings of the docs. Loop `docs` and read the content of each doc into the `corpus` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `corpus`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You expected to see:\n",
    "\n",
    "```['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']```\n",
    "\n",
    "But you actually saw:\n",
    "\n",
    "```['Ironhack is cool.', 'I love Ironhack.', 'I am a student at Ironhack.']```\n",
    "\n",
    "This is because you haven't done two important steps:\n",
    "\n",
    "1. Remove punctuation from the strings\n",
    "\n",
    "1. Convert strings to lowercase\n",
    "\n",
    "Write your code below to process `corpus` (convert to lower case and remove special characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']\n"
     ]
    }
   ],
   "source": [
    "corpus = [frase.replace('.','').lower() for frase in corpus] #substituindo a pontuação e aplicando lowercase\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define `bag_of_words` as an empty array. It will be used to store the unique terms in `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cool', 'student', 'at', 'am', 'i', 'love', 'ironhack', 'a', 'is']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bag_of_words = []\n",
    "\n",
    "for i in corpus:\n",
    "    \n",
    "    pv = re.findall(r\"\\S+\",i) #Acha as palavras e insere na variavel\n",
    "    bag_of_words.append(pv)\n",
    "    \n",
    "        \n",
    "bag_of_words = list(set().union(*bag_of_words)) #Unindo palavras e removendo repetidas por set\n",
    "\n",
    "\n",
    "\n",
    "print(bag_of_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through `corpus`. In each loop, do the following:\n",
    "\n",
    "1. Break the string into an array of terms. \n",
    "1. Create a sub-loop to iterate the terms array. \n",
    "  * In each sub-loop, you'll check if the current term is already contained in `bag_of_words`. If not in `bag_of_words`, append it to the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `bag_of_words`. You should see: \n",
    "\n",
    "```['ironhack', 'is', 'cool', 'i', 'love', 'am', 'a', 'student', 'at']```\n",
    "\n",
    "If not, fix your code in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define an empty array called `term_freq`. Loop `corpus` for a second time. In each loop, create a sub-loop to iterate the terms in `bag_of_words`. Count how many times each term appears in each doc of `corpus`. Append the term-frequency array to `term_freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag =  ['cool', 'student', 'at', 'am', 'i', 'love', 'ironhack', 'a', 'is']\n",
      "corpus =  ['ironhack is cool', 'i love ironhack', 'i am a student at ironhack']\n",
      "['ironhack', 'is', 'cool']\n",
      "['i', 'love', 'ironhack']\n",
      "['i', 'am', 'a', 'student', 'at', 'ironhack']\n",
      "[[1, 0, 0, 0, 0, 0, 1, 0, 1], [0, 0, 0, 0, 1, 1, 1, 0, 0], [0, 1, 1, 1, 1, 0, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "term_freq=[]\n",
    "\n",
    "\n",
    "\n",
    "for frase in corpus2:\n",
    "    p =[]\n",
    "    result = re.findall(r\"\\S+\",frase) # Procura palavras \\S+ e armazena a frase com a separação de palavras\n",
    "    \n",
    "    \n",
    "    for palavra in bag_of_words:\n",
    "        cont = result.count(palavra) # Contabiliza cada palavra na frase\n",
    "        p.append(cont)\n",
    "        \n",
    "    term_freq.append(p)\n",
    "    \n",
    "print(term_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If your output is correct, congratulations! You've solved the challenge!**\n",
    "\n",
    "If not, go back and check for errors in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question\n",
    "\n",
    "Optimize your solution for the above question by removing stop words from the BoW. For your convenience, a list of stop words is defined for you in the next cell. With the stop words removed, your output should look like:\n",
    "\n",
    "```\n",
    "bag_of_words = [am', 'at', 'cool', ironhack', 'is', 'love', 'student']\n",
    "\n",
    "term_freq = [\n",
    "\t[0, 0, 1, 1, 1, 0, 0],\n",
    " \t[0, 0, 0, 1, 0, 1, 0],\n",
    " \t[1, 1, 0, 1, 0, 0, 1]\n",
    "]\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Combine all your previous codes to the cell below.\n",
    "1. Improve your solution by ignoring stop words in `bag_of_words`.\n",
    "\n",
    "After you're done, your `bag_of_words` should be:\n",
    "\n",
    "```['ironhack', 'is', 'cool', 'love', 'am', 'student', 'at']```\n",
    "\n",
    "And your `term_freq` should be:\n",
    "\n",
    "```[[1, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 1, 1, 1]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Challenge for the Nerds\n",
    "\n",
    "We will learn Scikit-Learn in Module 3 which has built in the BoW feature. Try to use Scikit-Learn to generate the BoW for this challenge and check whether the output is the same as yours. You will need to do some googling to find out how to use Scikit-Learn to generate BoW.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* To install Scikit-Learn, use `pip install sklearn`. \n",
    "\n",
    "* Scikit-Learn removes stop words by default. You don't need to manually remove stop words.\n",
    "\n",
    "* Scikit-Learn's output has slightly different format from the output example demonstrated above. It's ok, you don't need to convert the Scikit-Learn output.\n",
    "\n",
    "The Scikit-Learn output will look like below:\n",
    "\n",
    "```python\n",
    "# BoW:\n",
    "{u'love': 5, u'ironhack': 3, u'student': 6, u'is': 4, u'cool': 2, u'am': 0, u'at': 1}\n",
    "\n",
    "# term_freq:\n",
    "[[0 0 1 1 1 0 0]\n",
    " [0 0 0 1 0 1 0]\n",
    " [1 1 0 1 0 0 1]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-382-6a1df9f4b4e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:root:code for hash md5 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type md5\n",
      "ERROR:root:code for hash sha1 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type sha1\n",
      "ERROR:root:code for hash sha224 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type sha224\n",
      "ERROR:root:code for hash sha256 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type sha256\n",
      "ERROR:root:code for hash sha384 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type sha384\n",
      "ERROR:root:code for hash sha512 was not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 147, in <module>\n",
      "    globals()[__func_name] = __get_hash(__func_name)\n",
      "  File \"/usr/local/Cellar/python@2/2.7.16/Frameworks/Python.framework/Versions/2.7/lib/python2.7/hashlib.py\", line 97, in __get_builtin_constructor\n",
      "    raise ValueError('unsupported hash type ' + name)\n",
      "ValueError: unsupported hash type sha512\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip\", line 11, in <module>\n",
      "    load_entry_point('pip==19.1.1', 'console_scripts', 'pip')()\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 489, in load_entry_point\n",
      "    return get_distribution(dist).load_entry_point(group, name)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2843, in load_entry_point\n",
      "    return ep.load()\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2434, in load\n",
      "    return self.resolve()\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 2440, in resolve\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_internal/__init__.py\", line 19, in <module>\n",
      "    from pip._vendor.urllib3.exceptions import DependencyWarning\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/__init__.py\", line 8, in <module>\n",
      "    from .connectionpool import (\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connectionpool.py\", line 29, in <module>\n",
      "    from .connection import (\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/connection.py\", line 38, in <module>\n",
      "    from .util.ssl_ import (\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/__init__.py\", line 6, in <module>\n",
      "    from .ssl_ import (\n",
      "  File \"/usr/local/lib/python2.7/site-packages/pip/_vendor/urllib3/util/ssl_.py\", line 8, in <module>\n",
      "    from hashlib import md5, sha1, sha256\n",
      "ImportError: cannot import name md5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
